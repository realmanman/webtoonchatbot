import os
import pandas as pd
import streamlit as st
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma, FAISS
from langchain_community.document_loaders import CSVLoader
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQAWithSourcesChain
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import ChatPromptTemplate

import re
from pathlib import Path

import sqlite3
from datetime import datetime

from langchain.chains import RetrievalQA
from langchain.memory import ConversationSummaryBufferMemory

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
####################################################################################

# OpenAI API Key ë“±ë¡
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤. .envì— ì„¤ì •í•´ ì£¼ì„¸ìš”.")

# LLM ì„¤ì •
llm = ChatOpenAI(model="gpt-4o", temperature=0, max_tokens=1024)

# DB ê²½ë¡œ
WEBTOON_CSV_PATH = "./webtoon_data.csv" # ì›¹íˆ° í†µê³„ ë²¡í„°DB
CHROMA_DIR = "./chroma_db5"      # ë²•ë¥ /ê°€ì´ë“œ ë¬¸ì„œ ë²¡í„°DB
FAISS_DB_DIR = "./db"            # webtoon_synopsis.csv ê¸°ë°˜ FAISS ë””ë ‰í† ë¦¬

@st.cache_resource
def build_or_load_vectorstores():
    # ---- 1) CSV ë¡œë“œ (í•„ìˆ˜ ì»¬ëŸ¼ ìœ íš¨ì„± ì ê²€) ----
    df = pd.read_csv(WEBTOON_CSV_PATH)
    if "ì¡°íšŒìˆ˜" in df.columns and "êµ¬ë…ììˆ˜" not in df.columns:
        df = df.rename(columns={"ì¡°íšŒìˆ˜": "êµ¬ë…ììˆ˜"})
    # ì•ˆì „í•œ ê¸°ë³¸ ì»¬ëŸ¼ ì²´í¬
    for col in ["ì œëª©", "í‰ì ", "êµ¬ë…ììˆ˜"]:
        if col not in df.columns:
            st.warning(f"CSVì— '{col}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì¼ë¶€ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆì–´ìš”.")

    # ---- 2) ë²•ë¥ /ê°€ì´ë“œ ë¬¸ì„œìš© Chroma ----
    # ì—†ìœ¼ë©´ ê·¸ëƒ¥ ë¹ˆ DBì—¬ë„ ë˜ì§€ë§Œ, ìµœì†Œ í´ë” ìƒì„±
    Path(CHROMA_DIR).mkdir(parents=True, exist_ok=True)
    embeddings_hf = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-base")
    vectordb = Chroma(persist_directory=CHROMA_DIR, embedding_function=embeddings_hf)
    retriever_law = vectordb.as_retriever(search_kwargs={"k": 10})
    rag_chain_law = RetrievalQAWithSourcesChain.from_chain_type(llm=llm, retriever=retriever_law)

    # ---- 3) ì›¹íˆ° ì •ë³´ìš© FAISS (ì—†ìœ¼ë©´ CSVì—ì„œ ì¦‰ì‹œ ë¹Œë“œ) ----
    faiss_path = Path(FAISS_DB_DIR)
    if faiss_path.exists() and any(faiss_path.iterdir()):
        embedding_info = OpenAIEmbeddings(model="text-embedding-3-small")
        vectorstore_info = FAISS.load_local(FAISS_DB_DIR, embedding_info, allow_dangerous_deserialization=True)
    else:
        # CSV ê° í–‰ì„ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ì„ë² ë”©í•˜ì—¬ ì •ë³´ ê²€ìƒ‰ ê°€ëŠ¥í•˜ê²Œ êµ¬ì„±
        records = []
        for _, row in df.iterrows():
            title = str(row.get("ì œëª©", ""))
            summary = str(row.get("ì¤„ê±°ë¦¬", row.get("í‚¤ì›Œë“œ", "")))
            meta = {k: (None if pd.isna(v) else v) for k, v in row.to_dict().items()}
            content = f"ì œëª©: {title}\nìš”ì•½: {summary}\në©”íƒ€: {meta}"
            records.append(content)
        embedding_info = OpenAIEmbeddings(model="text-embedding-3-small")
        vectorstore_info = FAISS.from_texts(records, embedding_info)
        vectorstore_info.save_local(FAISS_DB_DIR)

    retriever_info = vectorstore_info.as_retriever(search_kwargs={"k": 10})
    memory_info = ConversationBufferMemory(memory_key="chat_history", return_messages=True, output_key="answer")

    return df, rag_chain_law, vectordb, retriever_info, memory_info

df, rag_chain, vectordb, retriever_info, memory_info = build_or_load_vectorstores()

# ---- Intent/íŒŒì‹± ê³ ë„í™” ----
prompt_template_info = ChatPromptTemplate.from_messages([
    ("system", "ë„ˆëŠ” ì›¹íˆ° ì „ë¬¸ê°€ì•¼. ë°˜ë“œì‹œ ì›¹íˆ° ë°ì´í„°ë² ì´ìŠ¤(ê²€ìƒ‰ ë¬¸ì„œ) ë‚´ ì •ë³´ë§Œ ì‚¬ìš©í•´. \
ë¬¸ì„œê°€ ì—†ìœ¼ë©´ 'í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³ ë§Œ ë§í•´."),
    ("human", "ì§ˆë¬¸: {question}\n\nê´€ë ¨ ë¬¸ì„œ:\n{context}")
])

def custom_rag_chatbot(query, retriever, llm, memory):
    docs = retriever.get_relevant_documents(query)
    if not docs:
        return "í•´ë‹¹ ì •ë³´ë¥¼ ì›¹íˆ° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›¹íˆ°ê³¼ ê´€ë ¨ëœ ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."
    context = "\n\n".join([d.page_content for d in docs])
    messages = prompt_template_info.format_messages(question=query, context=context)
    response = llm(messages)
    memory.chat_memory.add_user_message(query)
    memory.chat_memory.add_ai_message(response.content)
    return response.content

# ì˜ë„ ë¶„ë¥˜ë¥¼ êµ¬ì¡°í™” ì¶œë ¥ìœ¼ë¡œ ê°•ì œ
def classify_intent(question: str) -> str:
    schema = """ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥: ë²•ë¥  | ì •ë³´ | í˜„í™© | ì¶”ì²œ
ê·œì¹™:
- 'ë²•ë¥ ': 2ì°¨ ì°½ì‘(ë“œë¼ë§ˆ/ì˜í™”/ê²Œì„/ì• ë‹ˆ ë“±) ì‹œì˜ ë²•ë¥ /ì €ì‘ê¶Œ/ê³„ì•½ ë“± ë²•ì  ì´ìŠˆ
- 'ì •ë³´': ì œëª©/ì¤„ê±°ë¦¬/ì‘ê°€/ë“±ì¥ì¸ë¬¼ ë“±ì˜ ì‘í’ˆ ìì²´ ì •ë³´
- 'ì¶”ì²œ': 2ì°¨ ì°½ì‘ ìš©ë„ë¡œ ì í•©í•œ ì›¹íˆ° ì¶”ì²œ ìš”êµ¬ (2ì°¨ ì°½ì‘ ì–¸ê¸‰ ì—†ìœ¼ë©´ ì¶”ì²œ ì•„ë‹˜)
- 'í˜„í™©': ì¡°íšŒìˆ˜/êµ¬ë…ììˆ˜/í‰ì /ìˆœìœ„/ì„ í˜¸ë„ ë“± í†µê³„/ë­í‚¹
ì§ˆë¬¸: """ + question + "\nì •ë‹µ:"
    ans = llm.predict(schema).strip()
    if ans not in {"ë²•ë¥ ", "ì •ë³´", "í˜„í™©", "ì¶”ì²œ"}:
        # fallback
        if any(k in question for k in ["ì¡°íšŒìˆ˜", "êµ¬ë…ììˆ˜", "í‰ì ", "ìˆœìœ„", "ì„ í˜¸ë„"]):
            return "í˜„í™©"
        if any(k in question for k in ["ì¶”ì²œ", "ê³¨ë¼ì¤˜", "ë­ê°€ ì¢‹ì•„"]):
            return "ì¶”ì²œ"
        return "ì •ë³´"
    return ans

def extract_top_k(question: str, default_k=10) -> int:
    # ìˆ«ì + (ê°œ|í¸|ëª…|ìœ„|top) íŒ¨í„´ ë‹¤ì–‘í™”
    m = re.search(r"(?:ìƒìœ„|top\s*)?(\d+)\s*(?:ê°œ|í¸|ëª…|ìœ„)?", question, re.IGNORECASE)
    if m:
        try:
            n = int(m.group(1))
            return max(1, min(20, n))
        except:
            pass
    return default_k

def extract_filter_conditions(question: str) -> dict:
    prompt = f"""ì§ˆë¬¸ì—ì„œ ì•„ë˜ ì¡°ê±´ì„ ì¶”ì¶œ.
- ì¹´í…Œê³ ë¦¬: ì¥ë¥´ (ë¡œë§¨ìŠ¤/ë¬´í˜‘/ë“œë¼ë§ˆ/ì•¡ì…˜/ìŠ¤ë¦´ëŸ¬ ë“±). ëª¨ë¥´ë©´ ë¹ˆì¹¸.
- ì—°ë ¹ì¸µ: 10~20ëŒ€ / 30ëŒ€ / 40ëŒ€ ì¤‘ íƒ1. ëª¨ë¥´ë©´ ë¹ˆì¹¸.
- ì„±ë³„: ë‚¨ì„± / ì—¬ì„± ì¤‘ íƒ1. ëª¨ë¥´ë©´ ë¹ˆì¹¸.

ì¶œë ¥ í˜•ì‹(ê·¸ëŒ€ë¡œ):
ì¹´í…Œê³ ë¦¬:
ì—°ë ¹ì¸µ:
ì„±ë³„:

ì§ˆë¬¸: "{question}"
"""
    result = llm.predict(prompt).strip()
    cond = {"ì¹´í…Œê³ ë¦¬": "", "ì—°ë ¹ì¸µ": "", "ì„±ë³„": ""}
    for line in result.splitlines():
        if ":" in line:
            k, v = [s.strip() for s in line.split(":", 1)]
            if k in cond:
                cond[k] = v
    return cond

def _safe_contains(series: pd.Series, needle: str) -> pd.Series:
    if not needle:
        return pd.Series([True]*len(series), index=series.index)
    return series.fillna("").astype(str).str.contains(needle, case=False, na=False)

def handle_recommendation(question: str) -> str:
    k = extract_top_k(question, default_k=3)
    cond = extract_filter_conditions(question)
    filtered = df.copy()

    if "ì¹´í…Œê³ ë¦¬" in df.columns and cond["ì¹´í…Œê³ ë¦¬"]:
        filtered = filtered[_safe_contains(filtered["ì¹´í…Œê³ ë¦¬"], cond["ì¹´í…Œê³ ë¦¬"])]

    # ì—°ë ¹ì¸µ/ì„±ë³„ ê´€ë ¨ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ìš°íšŒ
    if cond["ì—°ë ¹ì¸µ"] in df.columns:
        filtered = filtered[_safe_contains(filtered[cond["ì—°ë ¹ì¸µ"]], "ìƒ|ì¤‘")]
    if "ì„±ë³„ì„ í˜¸ë„" in df.columns and cond["ì„±ë³„"]:
        filtered = filtered[_safe_contains(filtered["ì„±ë³„ì„ í˜¸ë„"], cond["ì„±ë³„"])]

    if filtered.empty:
        return "â— ì¡°ê±´ì— ë§ëŠ” ì›¹íˆ°ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    # ê°„ë‹¨ ì ìˆ˜ì‹ (ê°€ì¤‘ì¹˜ ìƒìˆ˜ëŠ” ì„¤ì •ê°’í™” ê°€ëŠ¥)
    score = []
    for _, row in filtered.iterrows():
        rating = float(row.get("í‰ì ", 0) or 0)
        subs = float(row.get("êµ¬ë…ììˆ˜", 0) or 0)
        score.append(rating * 1.0 + (subs / 1000.0))
    filtered = filtered.assign(ì¶”ì²œì ìˆ˜=score).sort_values("ì¶”ì²œì ìˆ˜", ascending=False).head(k)

    lines = []
    for _, r in filtered.iterrows():
        title = r.get("ì œëª©", "N/A")
        rating = r.get("í‰ì ", "N/A")
        subs = int(float(r.get("êµ¬ë…ììˆ˜", 0)))
        kw = r.get("í‚¤ì›Œë“œ", "")
        lines.append(f"- {title} (í‰ì : {rating}, êµ¬ë…ììˆ˜: {subs}, í‚¤ì›Œë“œ: {kw})")
    return "\n".join(lines)

def handle_status(question: str) -> str:
    k = extract_top_k(question, default_k=3)
    cond = extract_filter_conditions(question)
    filtered = df.copy()

    if "ì¹´í…Œê³ ë¦¬" in df.columns and cond["ì¹´í…Œê³ ë¦¬"]:
        filtered = filtered[_safe_contains(filtered["ì¹´í…Œê³ ë¦¬"], cond["ì¹´í…Œê³ ë¦¬"])]
    if cond["ì—°ë ¹ì¸µ"] in df.columns:
        filtered = filtered[_safe_contains(filtered[cond["ì—°ë ¹ì¸µ"]], "ìƒ|ì¤‘")]
    if "ì„±ë³„ì„ í˜¸ë„" in df.columns and cond["ì„±ë³„"]:
        filtered = filtered[_safe_contains(filtered["ì„±ë³„ì„ í˜¸ë„"], cond["ì„±ë³„"])]

    # ì •ë ¬ ê¸°ì¤€ ìë™ íŒë³„
    if "êµ¬ë…ììˆ˜" in question:
        key = "êµ¬ë…ììˆ˜"
    elif "í‰ì " in question:
        key = "í‰ì "
    else:
        key = None

    if key and key in filtered.columns:
        top = filtered.sort_values(key, ascending=False).head(k)
    else:
        cols = [c for c in ["í‰ì ", "êµ¬ë…ììˆ˜"] if c in filtered.columns]
        if cols:
            top = filtered.sort_values(cols, ascending=False).head(k)
        else:
            return "â— ì¡°ê±´ì— ë§ëŠ” ì›¹íˆ°ì´ ì—†ìŠµë‹ˆë‹¤."

    lines = []
    for _, r in top.iterrows():
        title = r.get("ì œëª©", "N/A")
        rating = r.get("í‰ì ", "N/A")
        subs = int(float(r.get("êµ¬ë…ììˆ˜", 0)))
        cat = r.get("ì¹´í…Œê³ ë¦¬", "")
        lines.append(f"- {title} (í‰ì : {rating}, êµ¬ë…ììˆ˜: {subs}, ì¹´í…Œê³ ë¦¬: {cat})")
    return "\n".join(lines)



# Streamlit UI
st.title("ğŸ“š ì›¹íˆ° í†µí•© ì±—ë´‡ (ë²•ë¥  + ì •ë³´ + í˜„í™© + ì¶”ì²œ)")
st.caption("2ì°¨ ì°½ì‘ + ë°ì´í„° í†µê³„ + ì›¹íˆ° ì •ë³´ ì œê³µ ì±—ë´‡")

if "history" not in st.session_state:
    st.session_state.history = []

with st.form("input_form", clear_on_submit=True):
    user_input = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: 30ëŒ€ ì—¬ì„±ì´ ì¢‹ì•„í•  ë“œë¼ë§ˆ ì›¹íˆ° ì¶”ì²œí•´ì¤˜")
    submitted = st.form_submit_button("ì§ˆë¬¸í•˜ê¸°")

if submitted and user_input:
    intent = classify_intent(user_input)
    response = ""
    chunks = []

    if intent == "ë²•ë¥ ":
        result = rag_chain(user_input)
        response = result["answer"]
        chunks = vectordb.similarity_search(user_input, k=3)
    elif intent == "í˜„í™©":
        response = handle_status(user_input)
    elif intent == "ì¶”ì²œ":
        response = handle_recommendation(user_input)
    elif intent == "ì •ë³´":
        response = custom_rag_chatbot(user_input, retriever_info, llm, memory_info)
    else:
        response = "â— ì§ˆë¬¸ì„ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

    st.session_state.history.append({
        "ì§ˆë¬¸": user_input,
        "ë‹µë³€": response,
        "intent": intent,
        "chunks": chunks
    })

for chat in reversed(st.session_state.history):
    st.markdown(f"**ğŸ’¬ ì§ˆë¬¸:** {chat['ì§ˆë¬¸']}")
    if chat["intent"] == "ë²•ë¥ ":
        st.markdown("**ğŸ“š ë²•ë¥  ì‘ë‹µ:**")
    elif chat["intent"] == "í˜„í™©":
        st.markdown("**ğŸ“Š í˜„í™© ì‘ë‹µ:**")
    elif chat["intent"] == "ì¶”ì²œ":
        st.markdown("**ğŸ¯ ì¶”ì²œ ì‘ë‹µ:**")
    elif chat["intent"] == "ì •ë³´":
        st.markdown("**ğŸ“˜ ì›¹íˆ° ì •ë³´ ì‘ë‹µ:**")
    st.markdown(chat['ë‹µë³€'].replace("\n", "  \n"))
    st.divider()